# üöÄ PROJET 1 : Pipeline ETL Classique

## üìã Vue d'ensemble

Ce projet impl√©mente un **pipeline ETL (Extract-Transform-Load) classique** avec orchestration Airflow, permettant d'extraire des donn√©es depuis PostgreSQL et des fichiers CSV, de les transformer avec Pandas, puis de les charger dans un Data Warehouse structur√© en sch√©ma dimensionnel.

### üéØ Objectifs p√©dagogiques
- Comprendre l'approche ETL traditionnelle
- Orchestrer des pipelines avec Apache Airflow
- Impl√©menter un mod√®le dimensionnel (sch√©ma en √©toile)
- Visualiser les donn√©es avec Apache Superset

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL      ‚îÇ‚îÄ‚îÄ‚îê
‚îÇ Source (5433)   ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fichiers CSV/   ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Apache Airflow ‚îÇ
‚îÇ Excel (data/)   ‚îÇ  ‚îÇ    ‚îÇ   Orchestration  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ             ‚îÇ
                     ‚îÇ             ‚ñº
                     ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îî‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Transformation  ‚îÇ
                          ‚îÇ  (Pandas/Python) ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                                   ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ PostgreSQL DWH   ‚îÇ
                          ‚îÇ (5434)           ‚îÇ
                          ‚îÇ ‚îú‚îÄ staging       ‚îÇ
                          ‚îÇ ‚îî‚îÄ marts         ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ                     ‚îÇ
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇSuperset ‚îÇ         ‚îÇ  Adminer  ‚îÇ
                   ‚îÇ (8088)  ‚îÇ         ‚îÇ  (8081)   ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Composants du Projet

### Services Docker

| Service | Port | Description | Credentials |
|---------|------|-------------|-------------|
| **postgres-source** | 5433 | Base de donn√©es source (prod simul√©e) | user: `source_user` / pass: `source_pass` |
| **postgres-dwh** | 5434 | Data Warehouse (staging + marts) | user: `dwh_user` / pass: `dwh_pass` |
| **airflow-webserver** | 8080 | Interface web Airflow | user: `admin` / pass: `admin` |
| **airflow-scheduler** | - | Planificateur de t√¢ches Airflow | - |
| **superset** | 8088 | Outil de visualisation | user: `admin` / pass: `admin` |
| **adminer** | 8081 | Interface DB (comme phpMyAdmin) | - |

---

## üìÇ Structure des Fichiers

```
projet1-etl-classic/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml          # Configuration Docker Compose
‚îú‚îÄ‚îÄ README.md                   # Ce fichier
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ pg_source_init.sql      # Init base source avec donn√©es de test
‚îÇ   ‚îî‚îÄ‚îÄ pg_dwh_init.sql         # Init DWH (staging + marts + dim_date)
‚îÇ
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îî‚îÄ‚îÄ dag_etl_classic.py  # Pipeline ETL orchestr√©
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ external_sales.csv      # Fichier CSV externe (cr√©√© auto)
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ architecture.png        # Sch√©ma d'architecture
```

---

## üöÄ Installation et D√©marrage

### Pr√©requis
- **Windows 10/11** avec WSL2 activ√©
- **Docker Desktop** install√© et d√©marr√©
- **Au moins 8 GB de RAM** disponible pour Docker

### √âtape 1 : Cr√©er la structure des dossiers

```bash
# Ouvrir PowerShell ou CMD
mkdir projet1-etl-classic
cd projet1-etl-classic

# Cr√©er les sous-dossiers
mkdir sql
mkdir airflow\dags
mkdir airflow\logs
mkdir airflow\plugins
mkdir airflow\scripts
mkdir data
```

### √âtape 2 : Copier les fichiers

1. **docker-compose.yml** ‚Üí racine du projet
2. **pg_source_init.sql** ‚Üí dossier `sql/`
3. **pg_dwh_init.sql** ‚Üí dossier `sql/`
4. **dag_etl_classic.py** ‚Üí dossier `airflow/dags/`

### √âtape 3 : Configurer les permissions (Windows)

```bash
# Cr√©er un fichier .env √† la racine
echo AIRFLOW_UID=50000 > .env
```

### √âtape 4 : D√©marrer les services

```bash
# Lancer tous les conteneurs
docker compose up -d

# V√©rifier que tout est d√©marr√©
docker compose ps
```

‚è±Ô∏è **Attendez 2-3 minutes** que tous les services soient pr√™ts (surtout Airflow).

### √âtape 5 : V√©rifier les logs

```bash
# Logs Airflow
docker compose logs airflow-webserver -f

# Logs PostgreSQL Source
docker compose logs postgres-source

# Logs PostgreSQL DWH
docker compose logs postgres-dwh
```

---

## üéÆ Utilisation

### 1Ô∏è‚É£ Acc√©der √† Airflow

1. Ouvrir un navigateur : **http://localhost:8080**
2. Se connecter :
   - Username : `admin`
   - Password : `admin`
3. Activer le DAG `etl_classic_pipeline`
4. Cliquer sur le bouton "‚ñ∂Ô∏è Play" pour lancer manuellement

### 2Ô∏è‚É£ V√©rifier les donn√©es dans Adminer

1. Ouvrir : **http://localhost:8081**
2. Se connecter √† la **base source** :
   - Syst√®me : `PostgreSQL`
   - Serveur : `postgres-source`
   - Utilisateur : `source_user`
   - Mot de passe : `source_pass`
   - Base : `source_db`

3. Explorer les tables :
   - `public.customers` (15 clients)
   - `public.products` (15 produits)
   - `public.orders` (20 commandes)

4. Se connecter au **DWH** :
   - Serveur : `postgres-dwh`
   - Utilisateur : `dwh_user`
   - Mot de passe : `dwh_pass`
   - Base : `dwh_db`

5. V√©rifier les donn√©es :
   - Sch√©ma `staging.*` (donn√©es brutes)
   - Sch√©ma `marts.*` (mod√®le dimensionnel)

### 3Ô∏è‚É£ Ex√©cuter des requ√™tes SQL

```bash
# Se connecter au DWH
docker exec -it etl_postgres_dwh psql -U dwh_user -d dwh_db

# Exemples de requ√™tes
\dt staging.*        # Lister les tables de staging
\dt marts.*          # Lister les tables marts

# Voir les ventes par pays
SELECT * FROM marts.vw_sales_by_country LIMIT 10;

# Voir les KPIs
SELECT * FROM marts.vw_kpi_summary;

# Compter les ventes
SELECT COUNT(*) FROM marts.fact_sales;

# Quitter
\q
```

### 4Ô∏è‚É£ Visualiser avec Superset

1. Ouvrir : **http://localhost:8088**
2. Se connecter :
   - Username : `admin`
   - Password : `admin`

3. **Ajouter une connexion √† la base** :
   - Settings ‚Üí Database Connections ‚Üí + Database
   - Nom : `DWH Analytics`
   - SQLAlchemy URI : 
     ```
     postgresql://dwh_user:dwh_pass@postgres-dwh:5432/dwh_db
     ```

4. **Cr√©er un Dataset** :
   - Data ‚Üí Datasets ‚Üí + Dataset
   - Database : `DWH Analytics`
   - Schema : `marts`
   - Table : `vw_sales_by_country`

5. **Cr√©er un Chart** :
   - Charts ‚Üí + Chart
   - S√©lectionner le dataset
   - Choisir un type (Bar Chart, Time Series, etc.)

---

## üìä Pipeline ETL - D√©tails

### √âtapes du DAG

1. **extract_from_source** : Extrait depuis PostgreSQL source
2. **extract_from_files** : Extrait depuis CSV/Excel
3. **transform_data** : Transforme avec Pandas (nettoyage, calculs)
4. **load_to_staging** : Charge dans `staging.*`
5. **load_dimensions** : Peuple `dim_customer`, `dim_product`, `dim_date`
6. **load_facts** : Charge `fact_sales`
7. **validate_data** : V√©rifie la qualit√© des donn√©es

### Transformations appliqu√©es

- **Nettoyage** : emails en minuscules, espaces supprim√©s
- **Enrichissement** : cr√©ation de `full_name` pour les clients
- **Agr√©gation** : calculs de `net_amount`, `date_key`
- **Validation** : v√©rification des nulls, coh√©rence des cl√©s

---

## üß™ Tests et Validation

### Test 1 : V√©rifier l'extraction

```bash
# Lister les fichiers extraits
docker exec -it etl_airflow_webserver ls -lh /opt/airflow/data/
```

### Test 2 : Comparer les totaux

```sql
-- Dans la base source
SELECT COUNT(*) FROM public.orders;

-- Dans le DWH
SELECT COUNT(*) FROM marts.fact_sales;
```

### Test 3 : V√©rifier les vues

```sql
-- Ventes par cat√©gorie
SELECT * FROM marts.vw_sales_by_category;

-- Top 5 clients
SELECT 
    c.full_name,
    COUNT(f.sale_key) as nb_orders,
    SUM(f.net_amount) as total_spent
FROM marts.fact_sales f
JOIN marts.dim_customer c ON f.customer_key = c.customer_key
GROUP BY c.full_name
ORDER BY total_spent DESC
LIMIT 5;
```

---

## üîß Commandes Utiles

### Gestion Docker

```bash
# D√©marrer
docker compose up -d

# Arr√™ter
docker compose stop

# Supprimer (‚ö†Ô∏è efface les donn√©es)
docker compose down -v

# Red√©marrer un service
docker compose restart airflow-scheduler

# Voir les logs en temps r√©el
docker compose logs -f airflow-webserver
```

### Debugging Airflow

```bash
# Tester une t√¢che manuellement
docker exec -it etl_airflow_webserver airflow tasks test etl_classic_pipeline extract_from_source 2024-01-01

# V√©rifier la configuration
docker exec -it etl_airflow_webserver airflow config list

# R√©initialiser la DB Airflow
docker exec -it etl_airflow_webserver airflow db reset
```

### Acc√®s aux bases de donn√©es

```bash
# PostgreSQL Source
docker exec -it etl_postgres_source psql -U source_user -d source_db

# PostgreSQL DWH
docker exec -it etl_postgres_dwh psql -U dwh_user -d dwh_db
```

---

## üéì Concepts P√©dagogiques

### Pourquoi ETL (et pas ELT) ?

‚úÖ **Avantages ETL** :
- Donn√©es nettoy√©es avant stockage
- Moins d'espace disque requis
- Contr√¥le total sur les transformations
- Id√©al pour les bases de donn√©es traditionnelles

‚ùå **Inconv√©nients ETL** :
- Moins flexible (transformations fig√©es dans le code)
- Difficile de revenir aux donn√©es brutes
- Requiert plus de d√©veloppement Python

### Mod√®le Dimensionnel (Sch√©ma en √âtoile)

```
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ dim_date    ‚îÇ
        ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ fact_sales  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       ‚îÇ             ‚îÇ        ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ              ‚îÇ               ‚îÇ
‚ñº              ‚ñº               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇdim_      ‚îÇ ‚îÇdim_      ‚îÇ ‚îÇdim_      ‚îÇ
‚îÇcustomer  ‚îÇ ‚îÇproduct   ‚îÇ ‚îÇ...       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Dimensions** : Contexte (Qui ? Quoi ? Quand ?)
**Faits** : Mesures quantitatives (Combien ?)

---

## üêõ D√©pannage

### Probl√®me : Airflow ne d√©marre pas

```bash
# V√©rifier les logs
docker compose logs airflow-init

# R√©initialiser
docker compose down -v
docker compose up -d
```

### Probl√®me : DAG ne s'affiche pas

```bash
# V√©rifier que le fichier est bien pr√©sent
docker exec -it etl_airflow_webserver ls -l /opt/airflow/dags/

# V√©rifier les erreurs de syntaxe
docker exec -it etl_airflow_webserver python /opt/airflow/dags/dag_etl_classic.py
```

### Probl√®me : Connexion refus√©e PostgreSQL

```bash
# V√©rifier que le conteneur tourne
docker compose ps postgres-source

# Tester la connexion
docker exec -it etl_postgres_source pg_isready -U source_user
```

### Probl√®me : Port d√©j√† utilis√©

```bash
# Changer les ports dans docker-compose.yml
# Par exemple : "8080:8080" ‚Üí "8090:8080"
```

---

## üìà Am√©liorations Possibles

1. **Incr√©mental Loading** : Ne charger que les nouvelles donn√©es
2. **Gestion des erreurs** : Notification par email en cas d'√©chec
3. **Data Quality** : Int√©grer Great Expectations
4. **Performance** : Utiliser PySpark pour gros volumes
5. **Monitoring** : Ajouter Prometheus + Grafana
6. **CI/CD** : Tests automatiques avec pytest

---

## üìö Ressources

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Documentation Superset](https://superset.apache.org/docs/intro)
- [Mod√®le dimensionnel - Kimball](https://www.kimballgroup.com/)
- [PostgreSQL Best Practices](https://wiki.postgresql.org/wiki/Don%27t_Do_This)

---

## üìù Notes Importantes

‚ö†Ô∏è **Ce projet est √† des fins p√©dagogiques** :
- Pas de s√©curit√© renforc√©e (mots de passe en clair)
- Pas de gestion des secrets (utiliser Vault en prod)
- Pas de haute disponibilit√©
- Donn√©es de test uniquement

üí° **Pour la production** :
- Utiliser des services manag√©s (AWS RDS, etc.)
- Impl√©menter des secrets managers
- Ajouter des tests unitaires et d'int√©gration
- Mettre en place du monitoring
- Utiliser Kubernetes pour l'orchestration

---

## ‚úÖ Checklist de Validation

- [ ] Tous les conteneurs d√©marrent sans erreur
- [ ] Airflow accessible sur http://localhost:8080
- [ ] DAG visible et activable
- [ ] Pipeline s'ex√©cute sans erreur
- [ ] Donn√©es pr√©sentes dans `marts.fact_sales`
- [ ] Vues fonctionnelles (`vw_sales_by_country`, etc.)
- [ ] Superset accessible et connect√© au DWH
- [ ] Adminer permet de visualiser les donn√©es

---

## üéØ Prochaine √âtape : Projet 2 (ELT Moderne)

Une fois ce projet ma√Ætris√©, passez au **Projet 2** qui impl√©mente une approche **ELT moderne** avec :
- **Airbyte** pour l'ingestion
- **dbt** pour les transformations SQL
- **Prefect** pour l'orchestration
- **Metabase** pour la visualisation

**Diff√©rences cl√©s** :
- Load d'abord, Transform apr√®s (dans le DWH)
- Transformations d√©claratives en SQL
- Plus de flexibilit√© et agilit√©

---

## üë®‚Äçüíª Auteur

Projet p√©dagogique - Data Engineering Learning Path

Pour toute question ou am√©lioration, n'h√©sitez pas √† ouvrir une issue !

---

**Bon apprentissage ! üöÄ**