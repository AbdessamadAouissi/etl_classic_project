# ğŸš€ PROJET 1 : Pipeline ETL Classique

## ğŸ“‹ Vue d'ensemble

Ce projet implÃ©mente un **pipeline ETL (Extract-Transform-Load) classique** avec orchestration Airflow, permettant d'extraire des donnÃ©es depuis PostgreSQL et des fichiers CSV, de les transformer avec Pandas, puis de les charger dans un Data Warehouse structurÃ© en schÃ©ma dimensionnel.

### ğŸ¯ Objectifs pÃ©dagogiques
- Comprendre l'approche ETL traditionnelle
- Orchestrer des pipelines avec Apache Airflow
- ImplÃ©menter un modÃ¨le dimensionnel (schÃ©ma en Ã©toile)
- Visualiser les donnÃ©es avec Apache Superset

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL      â”‚â”€â”€â”
â”‚ Source (5433)   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fichiers CSV/   â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚   Apache Airflow â”‚
â”‚ Excel (data/)   â”‚  â”‚    â”‚   Orchestration  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚             â”‚
                     â”‚             â–¼
                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â””â”€â”€â”€â–¶â”‚  Transformation  â”‚
                          â”‚  (Pandas/Python) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ PostgreSQL DWH   â”‚
                          â”‚ (5434)           â”‚
                          â”‚ â”œâ”€ staging       â”‚
                          â”‚ â””â”€ marts         â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                     â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                   â”‚Superset â”‚         â”‚  Adminer  â”‚
                   â”‚ (8088)  â”‚         â”‚  (8081)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Composants du Projet

### Services Docker

| Service | Port | Description | Credentials |
|---------|------|-------------|-------------|
| **postgres-source** | 5433 | Base de donnÃ©es source (prod simulÃ©e) | user: `source_user` / pass: `source_pass` |
| **postgres-dwh** | 5434 | Data Warehouse (staging + marts) | user: `dwh_user` / pass: `dwh_pass` |
| **airflow-webserver** | 8080 | Interface web Airflow | user: `admin` / pass: `admin` |
| **airflow-scheduler** | - | Planificateur de tÃ¢ches Airflow | - |
| **superset** | 8088 | Outil de visualisation | user: `admin` / pass: `admin` |
| **adminer** | 8081 | Interface DB (comme phpMyAdmin) | - |

---

## ğŸ“‚ Structure des Fichiers

```
projet1-etl-classic/
â”‚
â”œâ”€â”€ docker-compose.yml          # Configuration Docker Compose
â”œâ”€â”€ README.md                   # Ce fichier
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ pg_source_init.sql      # Init base source avec donnÃ©es de test
â”‚   â””â”€â”€ pg_dwh_init.sql         # Init DWH (staging + marts + dim_date)
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ dag_etl_classic.py  # Pipeline ETL orchestrÃ©
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ external_sales.csv      # Fichier CSV externe (crÃ©Ã© auto)
â”‚
â””â”€â”€ docs/
    â””â”€â”€ architecture.png        # SchÃ©ma d'architecture
```

---

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis
- **Windows 10/11** avec WSL2 activÃ©
- **Docker Desktop** installÃ© et dÃ©marrÃ©
- **Au moins 8 GB de RAM** disponible pour Docker

### Ã‰tape 1 : CrÃ©er la structure des dossiers

```bash
# Ouvrir PowerShell ou CMD
mkdir projet1-etl-classic
cd projet1-etl-classic

# CrÃ©er les sous-dossiers
mkdir sql
mkdir airflow\dags
mkdir airflow\logs
mkdir airflow\plugins
mkdir airflow\scripts
mkdir data
```

### Ã‰tape 2 : Copier les fichiers

1. **docker-compose.yml** â†’ racine du projet
2. **pg_source_init.sql** â†’ dossier `sql/`
3. **pg_dwh_init.sql** â†’ dossier `sql/`
4. **dag_etl_classic.py** â†’ dossier `airflow/dags/`

### Ã‰tape 3 : Configurer les permissions (Windows)

```bash
# CrÃ©er un fichier .env Ã  la racine
echo AIRFLOW_UID=50000 > .env
```

### Ã‰tape 4 : DÃ©marrer les services

```bash
# Lancer tous les conteneurs
docker compose up -d

# VÃ©rifier que tout est dÃ©marrÃ©
docker compose ps
```

â±ï¸ **Attendez 2-3 minutes** que tous les services soient prÃªts (surtout Airflow).

### Ã‰tape 5 : VÃ©rifier les logs

```bash
# Logs Airflow
docker compose logs airflow-webserver -f

# Logs PostgreSQL Source
docker compose logs postgres-source

# Logs PostgreSQL DWH
docker compose logs postgres-dwh
```

---

## ğŸ® Utilisation

### 1ï¸âƒ£ AccÃ©der Ã  Airflow

1. Ouvrir un navigateur : **http://localhost:8080**
2. Se connecter :
   - Username : `admin`
   - Password : `admin`
3. Activer le DAG `etl_classic_pipeline`
4. Cliquer sur le bouton "â–¶ï¸ Play" pour lancer manuellement

### 2ï¸âƒ£ VÃ©rifier les donnÃ©es dans Adminer

1. Ouvrir : **http://localhost:8081**
2. Se connecter Ã  la **base source** :
   - SystÃ¨me : `PostgreSQL`
   - Serveur : `postgres-source`
   - Utilisateur : `source_user`
   - Mot de passe : `source_pass`
   - Base : `source_db`

3. Explorer les tables :
   - `public.customers` (15 clients)
   - `public.products` (15 produits)
   - `public.orders` (20 commandes)

4. Se connecter au **DWH** :
   - Serveur : `postgres-dwh`
   - Utilisateur : `dwh_user`
   - Mot de passe : `dwh_pass`
   - Base : `dwh_db`

5. VÃ©rifier les donnÃ©es :
   - SchÃ©ma `staging.*` (donnÃ©es brutes)
   - SchÃ©ma `marts.*` (modÃ¨le dimensionnel)

### 3ï¸âƒ£ ExÃ©cuter des requÃªtes SQL

```bash
# Se connecter au DWH
docker exec -it etl_postgres_dwh psql -U dwh_user -d dwh_db

# Exemples de requÃªtes
\dt staging.*        # Lister les tables de staging
\dt marts.*          # Lister les tables marts

# Voir les ventes par pays
SELECT * FROM marts.vw_sales_by_country LIMIT 10;

# Voir les KPIs
SELECT * FROM marts.vw_kpi_summary;

# Compter les ventes
SELECT COUNT(*) FROM marts.fact_sales;

# Quitter
\q
```

### 4ï¸âƒ£ Visualiser avec Superset

1. Ouvrir : **http://localhost:8088**
2. Se connecter :
   - Username : `admin`
   - Password : `admin`

3. **Ajouter une connexion Ã  la base** :
   - Settings â†’ Database Connections â†’ + Database
   - Nom : `DWH Analytics`
   - SQLAlchemy URI : 
     ```
     postgresql://dwh_user:dwh_pass@postgres-dwh:5432/dwh_db
     ```

4. **CrÃ©er un Dataset** :
   - Data â†’ Datasets â†’ + Dataset
   - Database : `DWH Analytics`
   - Schema : `marts`
   - Table : `vw_sales_by_country`

5. **CrÃ©er un Chart** :
   - Charts â†’ + Chart
   - SÃ©lectionner le dataset
   - Choisir un type (Bar Chart, Time Series, etc.)

---

## ğŸ“Š Pipeline ETL - DÃ©tails

### Ã‰tapes du DAG

1. **extract_from_source** : Extrait depuis PostgreSQL source
2. **extract_from_files** : Extrait depuis CSV/Excel
3. **transform_data** : Transforme avec Pandas (nettoyage, calculs)
4. **load_to_staging** : Charge dans `staging.*`
5. **load_dimensions** : Peuple `dim_customer`, `dim_product`, `dim_date`
6. **load_facts** : Charge `fact_sales`
7. **validate_data** : VÃ©rifie la qualitÃ© des donnÃ©es

### Transformations appliquÃ©es

- **Nettoyage** : emails en minuscules, espaces supprimÃ©s
- **Enrichissement** : crÃ©ation de `full_name` pour les clients
- **AgrÃ©gation** : calculs de `net_amount`, `date_key`
- **Validation** : vÃ©rification des nulls, cohÃ©rence des clÃ©s

---

## ğŸ§ª Tests et Validation

### Test 1 : VÃ©rifier l'extraction

```bash
# Lister les fichiers extraits
docker exec -it etl_airflow_webserver ls -lh /opt/airflow/data/
```

### Test 2 : Comparer les totaux

```sql
-- Dans la base source
SELECT COUNT(*) FROM public.orders;

-- Dans le DWH
SELECT COUNT(*) FROM marts.fact_sales;
```

### Test 3 : VÃ©rifier les vues

```sql
-- Ventes par catÃ©gorie
SELECT * FROM marts.vw_sales_by_category;

-- Top 5 clients
SELECT 
    c.full_name,
    COUNT(f.sale_key) as nb_orders,
    SUM(f.net_amount) as total_spent
FROM marts.fact_sales f
JOIN marts.dim_customer c ON f.customer_key = c.customer_key
GROUP BY c.full_name
ORDER BY total_spent DESC
LIMIT 5;
```

---

## ğŸ”§ Commandes Utiles

### Gestion Docker

```bash
# DÃ©marrer
docker compose up -d

# ArrÃªter
docker compose stop

# Supprimer (âš ï¸ efface les donnÃ©es)
docker compose down -v

# RedÃ©marrer un service
docker compose restart airflow-scheduler

# Voir les logs en temps rÃ©el
docker compose logs -f airflow-webserver
```

### Debugging Airflow

```bash
# Tester une tÃ¢che manuellement
docker exec -it etl_airflow_webserver airflow tasks test etl_classic_pipeline extract_from_source 2024-01-01

# VÃ©rifier la configuration
docker exec -it etl_airflow_webserver airflow config list

# RÃ©initialiser la DB Airflow
docker exec -it etl_airflow_webserver airflow db reset
```

### AccÃ¨s aux bases de donnÃ©es

```bash
# PostgreSQL Source
docker exec -it etl_postgres_source psql -U source_user -d source_db

# PostgreSQL DWH
docker exec -it etl_postgres_dwh psql -U dwh_user -d dwh_db
```

---

## ğŸ“ Concepts PÃ©dagogiques

### Pourquoi ETL (et pas ELT) ?

âœ… **Avantages ETL** :
- DonnÃ©es nettoyÃ©es avant stockage
- Moins d'espace disque requis
- ContrÃ´le total sur les transformations
- IdÃ©al pour les bases de donnÃ©es traditionnelles

âŒ **InconvÃ©nients ETL** :
- Moins flexible (transformations figÃ©es dans le code)
- Difficile de revenir aux donnÃ©es brutes
- Requiert plus de dÃ©veloppement Python

### ModÃ¨le Dimensionnel (SchÃ©ma en Ã‰toile)

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ dim_date    â”‚
        â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¤ fact_sales  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       â”‚             â”‚        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚              â”‚               â”‚
â–¼              â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚dim_      â”‚ â”‚dim_      â”‚ â”‚dim_      â”‚
â”‚customer  â”‚ â”‚product   â”‚ â”‚...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dimensions** : Contexte (Qui ? Quoi ? Quand ?)
**Faits** : Mesures quantitatives (Combien ?)

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : Airflow ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker compose logs airflow-init

# RÃ©initialiser
docker compose down -v
docker compose up -d
```

### ProblÃ¨me : DAG ne s'affiche pas

```bash
# VÃ©rifier que le fichier est bien prÃ©sent
docker exec -it etl_airflow_webserver ls -l /opt/airflow/dags/

# VÃ©rifier les erreurs de syntaxe
docker exec -it etl_airflow_webserver python /opt/airflow/dags/dag_etl_classic.py
```

### ProblÃ¨me : Connexion refusÃ©e PostgreSQL

```bash
# VÃ©rifier que le conteneur tourne
docker compose ps postgres-source

# Tester la connexion
docker exec -it etl_postgres_source pg_isready -U source_user
```

### ProblÃ¨me : Port dÃ©jÃ  utilisÃ©

```bash
# Changer les ports dans docker-compose.yml
# Par exemple : "8080:8080" â†’ "8090:8080"
```

---

## ğŸ“ˆ AmÃ©liorations Possibles

1. **IncrÃ©mental Loading** : Ne charger que les nouvelles donnÃ©es
2. **Gestion des erreurs** : Notification par email en cas d'Ã©chec
3. **Data Quality** : IntÃ©grer Great Expectations
4. **Performance** : Utiliser PySpark pour gros volumes
5. **Monitoring** : Ajouter Prometheus + Grafana
6. **CI/CD** : Tests automatiques avec pytest

---

## ğŸ“š Ressources

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Documentation Superset](https://superset.apache.org/docs/intro)
- [ModÃ¨le dimensionnel - Kimball](https://www.kimballgroup.com/)
- [PostgreSQL Best Practices](https://wiki.postgresql.org/wiki/Don%27t_Do_This)

---

## ğŸ“ Notes Importantes

âš ï¸ **Ce projet est Ã  des fins pÃ©dagogiques** :
- Pas de sÃ©curitÃ© renforcÃ©e (mots de passe en clair)
- Pas de gestion des secrets (utiliser Vault en prod)
- Pas de haute disponibilitÃ©
- DonnÃ©es de test uniquement

ğŸ’¡ **Pour la production** :
- Utiliser des services managÃ©s (AWS RDS, etc.)
- ImplÃ©menter des secrets managers
- Ajouter des tests unitaires et d'intÃ©gration
- Mettre en place du monitoring
- Utiliser Kubernetes pour l'orchestration

---

## âœ… Checklist de Validation

- [ ] Tous les conteneurs dÃ©marrent sans erreur
- [ ] Airflow accessible sur http://localhost:8080
- [ ] DAG visible et activable
- [ ] Pipeline s'exÃ©cute sans erreur
- [ ] DonnÃ©es prÃ©sentes dans `marts.fact_sales`
- [ ] Vues fonctionnelles (`vw_sales_by_country`, etc.)
- [ ] Superset accessible et connectÃ© au DWH
- [ ] Adminer permet de visualiser les donnÃ©es

---

## ğŸ¯ Prochaine Ã‰tape : Projet 2 (ELT Moderne)

Une fois ce projet maÃ®trisÃ©, passez au **Projet 2** qui implÃ©mente une approche **ELT moderne** avec :
- **Airbyte** pour l'ingestion
- **dbt** pour les transformations SQL
- **Prefect** pour l'orchestration
- **Metabase** pour la visualisation

**DiffÃ©rences clÃ©s** :
- Load d'abord, Transform aprÃ¨s (dans le DWH)
- Transformations dÃ©claratives en SQL
- Plus de flexibilitÃ© et agilitÃ©

---

## ğŸ‘¨â€ğŸ’» Auteur

Projet pÃ©dagogique - Data Engineering Learning Path

Pour toute question ou amÃ©lioration, n'hÃ©sitez pas Ã  ouvrir une issue !

---

**Bon apprentissage ! ğŸš€**